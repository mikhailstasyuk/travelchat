{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import emoji \n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635da615",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/2025-05-21-travelask/result.json\"\n",
    "df = pd.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_messages = pd.json_normalize(df.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import emoji # Make sure to install: pip install emoji\n",
    "from tqdm.auto import tqdm # Using tqdm.auto for flexible environment (notebook, console)\n",
    "\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- Text Parsing and Analysis Helpers ---\n",
    "\n",
    "def _flatten_message_elements(data_list):\n",
    "    elements = []\n",
    "    if not data_list: # Handle empty list input\n",
    "        return elements\n",
    "    # Check if the first level is a list of lists (common in some message structures)\n",
    "    if all(isinstance(item, list) for item in data_list):\n",
    "        for sublist in data_list:\n",
    "            elements.extend(sublist)\n",
    "    else:\n",
    "        elements.extend(data_list)\n",
    "    return elements\n",
    "\n",
    "def parse_message_text_robust(text_column_entry):\n",
    "    parsed_content = None\n",
    "    # Order of checks is important to avoid pd.isna() on list/dict.\n",
    "    if isinstance(text_column_entry, (list, dict)):\n",
    "        parsed_content = text_column_entry  # Already a list/dict\n",
    "    elif pd.isna(text_column_entry): # Handles np.nan, None\n",
    "        return []\n",
    "    elif isinstance(text_column_entry, str):\n",
    "        try:\n",
    "            stripped_text = text_column_entry.strip()\n",
    "            # Attempt to parse only if it looks like a JSON list/object\n",
    "            if (stripped_text.startswith('[') and stripped_text.endswith(']')) or \\\n",
    "               (stripped_text.startswith('{') and stripped_text.endswith('}')):\n",
    "                parsed_content = json.loads(stripped_text)\n",
    "            else:\n",
    "                # It's a plain string\n",
    "                return [text_column_entry]\n",
    "        except json.JSONDecodeError:\n",
    "            # Failed to parse as JSON, treat as a plain string\n",
    "            return [text_column_entry]\n",
    "    else:\n",
    "        # Other scalar types (int, float, bool etc.), convert to string and wrap in list\n",
    "        return [str(text_column_entry)]\n",
    "\n",
    "    # Ensure consistent output format (list of elements)\n",
    "    if isinstance(parsed_content, dict): # If top level was a dict\n",
    "        return [parsed_content]\n",
    "    elif isinstance(parsed_content, list):\n",
    "        return _flatten_message_elements(parsed_content)\n",
    "    else:\n",
    "        # If json.loads returned a scalar (e.g. \"null\", \"true\", \"123\" as JSON)\n",
    "        return [str(parsed_content)] if parsed_content is not None else []\n",
    "\n",
    "\n",
    "def analyze_text_content(elements):\n",
    "    plain_text_parts = []\n",
    "    link_count = 0\n",
    "    mention_count = 0\n",
    "    text_link_count = 0\n",
    "    other_format_count = 0 # bold, italic, etc. (kept for potential future use, not used by current filters)\n",
    "\n",
    "    for item in elements:\n",
    "        if isinstance(item, str):\n",
    "            plain_text_parts.append(item)\n",
    "        elif isinstance(item, dict):\n",
    "            item_type = item.get('type', '').lower()\n",
    "            # Ensure 'text' value is a string, as it could be other types in malformed data\n",
    "            text_in_dict = item.get('text')\n",
    "            if not isinstance(text_in_dict, str) and text_in_dict is not None:\n",
    "                text_in_dict = str(text_in_dict)\n",
    "            elif text_in_dict is None:\n",
    "                text_in_dict = \"\"\n",
    "\n",
    "            if item_type == 'mention':\n",
    "                mention_count += 1\n",
    "            elif item_type == 'text_link':\n",
    "                text_link_count += 1\n",
    "            elif item_type == 'link':\n",
    "                link_count += 1\n",
    "            elif item_type in ['bold', 'italic', 'underline', 'strikethrough', 'code', 'pre']:\n",
    "                other_format_count +=1\n",
    "            \n",
    "            if text_in_dict: # Add text from any dict element if it exists\n",
    "                plain_text_parts.append(text_in_dict)\n",
    "    \n",
    "    full_text = \" \".join(plain_text_parts).strip() # Strip leading/trailing whitespace from combined text\n",
    "    interactive_elements_count = link_count + mention_count + text_link_count\n",
    "\n",
    "    return {\n",
    "        'full_text_original_case': full_text,\n",
    "        'full_text_lower': full_text.lower(),\n",
    "        'link_count': link_count, # Kept for completeness, summed into interactive_elements_count\n",
    "        'mention_count': mention_count, # Kept for completeness\n",
    "        'text_link_count': text_link_count, # Kept for completeness\n",
    "        'interactive_elements_count': interactive_elements_count,\n",
    "        'other_format_count': other_format_count\n",
    "    }\n",
    "\n",
    "# --- Craigslist-like Message Detection Logic ---\n",
    "\n",
    "CRAIGSLIST_KEYWORDS = [\n",
    "    \"куплю\", \"купить\", \"покупка\",\n",
    "    \"продам\", \"продать\", \"продаю\", \"продается\",\n",
    "    \"сдам\", \"сдаю\", \"сдается\", # rent out\n",
    "    \"сниму\", \"возьму в аренду\", \"арендую\", # rent\n",
    "    \"обменяю\", \"обмен\",\n",
    "    \"ищу\", # can be for job, item, service\n",
    "    \"предлагаю\", \"оказываю услуги\", \"услуги\",\n",
    "    \"требуется\", \"вакансия\", \"ищем сотрудника\", \"работа\", \"подработка\", \"фриланс\",\n",
    "    \"резюме\", \"ищу работу\", \"портфолио\"\n",
    "]\n",
    "\n",
    "PRICE_REGEX = r\"\\b(\\d{2,}(?:[.,]\\d+)?)\\s*(руб|usd|eur|gel|лари|тенге|kzt|btc|eth|usdt|₽|\\$|€|byn|uah|amd|azn|uzs|kgs|tjs|tmr|byn)\\b\"\n",
    "PRICE_KEYWORDS = [\"цена\", \"стоимость\", \"прайс\", \"оплата\", \"бюджет\", \"зарплата\", \"зп\"]\n",
    "\n",
    "def is_craigslist_like_message(text_column_entry):\n",
    "    elements = parse_message_text_robust(text_column_entry)\n",
    "    \n",
    "    if not elements or (len(elements) == 1 and isinstance(elements[0], str) and not elements[0].strip()):\n",
    "        return False # Empty or whitespace-only string after parsing\n",
    "        \n",
    "    analysis = analyze_text_content(elements)\n",
    "    text_lower = analysis['full_text_lower']\n",
    "    original_text = analysis['full_text_original_case']\n",
    "\n",
    "    if not text_lower.strip() and analysis['interactive_elements_count'] == 0: # No text and no links/mentions\n",
    "        return False\n",
    "\n",
    "    # Craigslist-style ads detection (adapted from original Rule 3)\n",
    "    has_craigslist_keyword = any(keyword in text_lower for keyword in CRAIGSLIST_KEYWORDS)\n",
    "    if has_craigslist_keyword:\n",
    "        has_price_signal = any(kw in text_lower for kw in PRICE_KEYWORDS) or re.search(PRICE_REGEX, text_lower, re.IGNORECASE)\n",
    "        has_contact_phrase = any(phrase in text_lower for phrase in [\"писать в лс\", \"пишите в личку\", \"звоните\", \"обращайтесь\", \"подробности по телефону\", \"мой контакт\", \"связь со мной\"])\n",
    "        \n",
    "        # If a craigslist keyword is present with a clear call to action or price/contact info\n",
    "        if analysis['interactive_elements_count'] >= 1 or has_price_signal or has_contact_phrase:\n",
    "            return True\n",
    "            \n",
    "        # If it's a longer message with a strong craigslist keyword, it's also suspicious,\n",
    "        # even without explicit contact/price IF other signals were absent.\n",
    "        strong_craigslist_kws = [\"продам\", \"сдам\", \"куплю\", \"вакансия\", \"требуется\", \"продается\", \"сдается\"]\n",
    "        # This condition checks if the previous 'if' was false (i.e., no interactive elements, no price, no contact phrase)\n",
    "        if analysis['interactive_elements_count'] == 0 and not has_price_signal and not has_contact_phrase:\n",
    "            if len(original_text) > 70 and any(kw in text_lower for kw in strong_craigslist_kws):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# --- Generic/Emoji Message Helpers ---\n",
    "\n",
    "GENERIC_PHRASES_LIST = [\n",
    "    \"всем привет\", \"привет\", \"здрасьте\", \"здравствуйте\", \"салют\", \"ку\",\n",
    "    \"доброе утро\", \"добрый день\", \"добрый вечер\", \"доброй ночи\", \"ду\", \"дд\", \"дв\", \"дн\",\n",
    "    \"спс\", \"спасибо\", \"спсб\", \"благодарю\", \"мерси\", \"благодарочка\", \"пасиб\", \"thx\", \"ty\",\n",
    "    \"пожалуйста\", \"пж\", \"плз\", \"что\", \"чтобы что\",\"не за что\", \"нзч\", \"welcome\", \"yw\",\n",
    "    \"ок\", \"окей\", \"оке\", \"ладно\", \"хорошо\", \"хорош\", \"гуд\", \"согласен\", \"согласна\", \"договорились\", \"добро\", \"конечно\", \"yes\", \"да\", \"да ну\",\n",
    "    \"понял\", \"поняла\", \"понятно\", \"ясно\", \"принял\", \"приняла\", \"принято\", \"яснопонятно\", \"got it\",\n",
    "    \"лол\", \"кек\", \"рофл\", \"ахах\", \"ахахах\", \"хаха\", \"хах\", \"хех\", \"гг\", \"ыы\", \"ору\", \"ор\", \"жиза\", \"lol\", \"kek\", \"rofl\",\n",
    "    \"всего доброго\", \"всего хорошего\", \"удачи\", \"добра\", \"бб\", \"bb\",\n",
    "    \"до свидания\", \"пока\", \"бай\", \"чао\", \"до встречи\",\n",
    "    \"нет\", \"ага\", \"угу\", \"неа\", \"ноуп\", \"no\", \"nope\", \"ну\", \"ну да\",\n",
    "    \"+\", \"++\", \"-\", \"--\", \"+++\", \"---\",\n",
    "    \"супер\", \"класс\", \"отлично\", \"круто\", \"круть\", \"кайф\", \"кайфы\", \"кул\", \"огонь\", \"топ\", \"бомба\", \"пушка\", \"найс\", \"nice\", \"cool\", \"perfecto\",\n",
    "    \"f\", \"rip\", \n",
    "    \"ап\", \"up\", \"bump\",\n",
    "    \"сорян\", \"сори\", \"сорри\", \"извини\", \"извините\", \"извиняюсь\", \"прости\", \"простите\", \"sorry\", \"sry\",\n",
    "    \"хз\", \"хзхз\", \"не знаю\", \"без понятия\", \"idk\",\n",
    "    \"мда\", \"мде\", \"эх\", \"ого\", \"ухты\", \"вау\", \"wow\",\n",
    "    \"минутку\", \"сек\", \"сейчас\", \"погодь\", \"подожди\"\n",
    "]\n",
    "NORMALIZED_GENERIC_PHRASES = set(GENERIC_PHRASES_LIST)\n",
    "\n",
    "\n",
    "def get_plain_text_from_message(text_column_entry):\n",
    "    elements = parse_message_text_robust(text_column_entry)\n",
    "    plain_text_parts = []\n",
    "    for item in elements:\n",
    "        if isinstance(item, str):\n",
    "            plain_text_parts.append(item)\n",
    "        elif isinstance(item, dict) and 'text' in item:\n",
    "            text_val = item['text']\n",
    "            if isinstance(text_val, str):\n",
    "                plain_text_parts.append(text_val)\n",
    "            elif text_val is not None:\n",
    "                plain_text_parts.append(str(text_val))\n",
    "    return \" \".join(plain_text_parts).strip()\n",
    "\n",
    "\n",
    "def is_emoji_only(text: str) -> bool:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    text_no_space = \"\".join(text.split()) # Remove all whitespace\n",
    "    if not text_no_space: # Original text was only whitespace\n",
    "        return False\n",
    "\n",
    "    demojized_text = emoji.demojize(text_no_space)\n",
    "    if re.fullmatch(r\"(:[a-zA-Z0-9_+\\-]+(?:_face_with_[a-zA-Z0-9_+\\-]+)?(?:_type_[1-6])?:)+\", demojized_text):\n",
    "        return True\n",
    "    \n",
    "    try: \n",
    "        if emoji.emoji_count(text_no_space) > 0 and emoji.replace_emoji(text_no_space, replace='') == '':\n",
    "            return True\n",
    "    except AttributeError: \n",
    "        try: \n",
    "            is_all_emoji = True\n",
    "            if not text_no_space: return False \n",
    "            for char_in_text in text_no_space:\n",
    "                if not emoji.is_emoji(char_in_text):\n",
    "                    is_all_emoji = False\n",
    "                    break\n",
    "            if is_all_emoji:\n",
    "                return True\n",
    "        except AttributeError: \n",
    "            pass \n",
    "            \n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        \"\\U00002B05-\\U00002B07\"  # Arrows\n",
    "        \"\\U00002934-\\U00002935\"  # Arrows\n",
    "        \"\\U00003297-\\U00003299\"  # Enclosed Characters\n",
    "        \"\\U00003030\"             # Wavy Dash\n",
    "        \"\\U000000A9-\\U000000AE\"  # Copyright, Registered Sign, TM\n",
    "        \"\\U00002122\"             # TM\n",
    "        \"\\U0000231A-\\U0000231B\"  # Watch, Hourglass\n",
    "        \"\\U0000200D\"             # Zero Width Joiner (part of sequences)\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    if emoji_pattern.fullmatch(text_no_space): \n",
    "        return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "\n",
    "def is_generic_or_emoji_message(text_column_entry):\n",
    "    plain_text = get_plain_text_from_message(text_column_entry)\n",
    "\n",
    "    # 1. Handle empty or whitespace-only messages\n",
    "    if not plain_text.strip():\n",
    "        return True\n",
    "\n",
    "    # 2. Handle emoji-only messages\n",
    "    if is_emoji_only(plain_text):\n",
    "        return True\n",
    "\n",
    "    # 3. Normalization for generic phrase check\n",
    "    text_to_check = plain_text.lower()\n",
    "\n",
    "    # 3a. Remove trailing parentheses (common in Russian for smileys)\n",
    "    text_to_check = re.sub(r\"[()]+$\", \"\", text_to_check).strip()\n",
    "    \n",
    "    # 3b. Remove all emojis from the text\n",
    "    try:\n",
    "        text_to_check = emoji.replace_emoji(text_to_check, replace='')\n",
    "    except AttributeError: # Fallback if replace_emoji is not available\n",
    "        text_to_check = emoji.demojize(text_to_check)\n",
    "        text_to_check = re.sub(r\":[^:\\s]+:\", \"\", text_to_check) # Remove demojized emojis\n",
    "        text_to_check = emoji.emojize(text_to_check) # Convert back if any non-emoji text remains\n",
    "\n",
    "    # 3c. Strip again in case emoji removal left spaces or if it became empty\n",
    "    text_to_check = text_to_check.strip()\n",
    "\n",
    "    if not text_to_check: # If only emojis and/or trailing parentheses were present\n",
    "        return False # Not a generic *phrase* if what's left is empty\n",
    "\n",
    "    # 3d. Standard punctuation and space normalization\n",
    "    text_to_check = re.sub(r\"\"\"[\\s!\"#$%&'()*+,.\\-/:;<=>?@[\\\\\\]^_`{|}~]+\"\"\", \" \", text_to_check).strip()\n",
    "    text_to_check = re.sub(r'\\s+', ' ', text_to_check).strip()\n",
    "\n",
    "\n",
    "    # 4. Check against normalized generic phrases\n",
    "    if text_to_check in NORMALIZED_GENERIC_PHRASES:\n",
    "        return True\n",
    "    \n",
    "    # Handle cases like \"+++\" or \"---\" which become \"+ +\" or \"- -\" after regex\n",
    "    if text_to_check in [\"+ +\", \"- -\", \"+ + +\", \"- - -\"]: \n",
    "        return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "\n",
    "BANNED_FROM_NAMES = [\n",
    "    'travelask_moderator_bot',\n",
    "    'NoName👽NotAbot',\n",
    "    '@travelask_help_bot',\n",
    "    'Trusted_recommendation_bot',\n",
    "    'Анна Богомолова Чат-бот в ТГ, Getcourse, Salebot       27' # Note the trailing spaces\n",
    "]\n",
    "\n",
    "def preprocess_messages_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_processed = df.copy()\n",
    "    initial_rows = len(df_processed)\n",
    "\n",
    "    if initial_rows == 0:\n",
    "        print(\"Initial DataFrame is empty. No preprocessing to perform.\")\n",
    "        return df_processed\n",
    "        \n",
    "    print(f\"Initial number of messages: {initial_rows}\")\n",
    "\n",
    "    # Step 1: Remove all rows where type == \"service\"\n",
    "    if 'type' in df_processed.columns:\n",
    "        rows_before = len(df_processed)\n",
    "        df_processed = df_processed[df_processed['type'] != 'service'].copy()\n",
    "        print(f\"Step 1: Removed {rows_before - len(df_processed)} service messages. Rows remaining: {len(df_processed)}\")\n",
    "    else:\n",
    "        print(\"Warning: 'type' column not found. Skipping Step 1 (remove service messages).\")\n",
    "\n",
    "    # Step 2: Remove messages from specified 'from' names\n",
    "    if 'from' in df_processed.columns and not df_processed.empty:\n",
    "        rows_before = len(df_processed)\n",
    "        # Ensure 'from' column is treated as string for comparison, handling potential NaNs\n",
    "        # .isin() handles NaN correctly (NaN.isin(list) is False unless NaN is in list)\n",
    "        df_processed = df_processed[~df_processed['from'].isin(BANNED_FROM_NAMES)].copy()\n",
    "        print(f\"Step 2: Removed {rows_before - len(df_processed)} messages from banned 'from' names. Rows remaining: {len(df_processed)}\")\n",
    "    elif df_processed.empty:\n",
    "        print(\"DataFrame is empty before 'from' name removal. Skipping Step 2.\")\n",
    "    else:\n",
    "        print(\"Warning: 'from' column not found. Skipping Step 2 (remove messages from banned 'from' names).\")\n",
    "\n",
    "    # Step 3: Identify and remove Craigslist-like messages\n",
    "    if 'text' in df_processed.columns and not df_processed.empty:\n",
    "        rows_before = len(df_processed)\n",
    "        print(\"Step 3: Identifying and removing Craigslist-like messages...\")\n",
    "        craigslist_mask = df_processed['text'].progress_apply(is_craigslist_like_message)\n",
    "        df_processed = df_processed[~craigslist_mask].copy()\n",
    "        print(f\"Step 3: Removed {rows_before - len(df_processed)} Craigslist-like messages. Rows remaining: {len(df_processed)}\")\n",
    "    elif df_processed.empty:\n",
    "        print(\"DataFrame is empty before Craigslist-like message removal. Skipping Step 3.\")\n",
    "    else:\n",
    "        print(\"Warning: 'text' column not found. Skipping Craigslist-like message removal (Step 3).\")\n",
    "\n",
    "    # Step 4: Remove generic greetings/thanks, emoji-only, and EMPTY messages\n",
    "    if 'text' in df_processed.columns and not df_processed.empty:\n",
    "        rows_before = len(df_processed)\n",
    "        print(\"Step 4: Identifying and removing generic/emoji-only/empty messages...\")\n",
    "        generic_mask = df_processed['text'].progress_apply(is_generic_or_emoji_message)\n",
    "        df_processed = df_processed[~generic_mask].copy()\n",
    "        print(f\"Step 4: Removed {rows_before - len(df_processed)} generic/emoji-only/empty messages. Rows remaining: {len(df_processed)}\")\n",
    "    elif df_processed.empty:\n",
    "        print(\"DataFrame is empty before generic/emoji/empty message removal. Skipping Step 4.\")\n",
    "    else:\n",
    "        print(\"Warning: 'text' column not found. Skipping generic/emoji/empty message removal (Step 4).\")\n",
    "    \n",
    "    final_rows = len(df_processed)\n",
    "    print(f\"Preprocessing complete. Total messages removed: {initial_rows - final_rows}. Final number of messages: {final_rows}\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0134274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = preprocess_messages_dataframe(df_all_messages)\n",
    "print(f\"Original DataFrame shape: {df_all_messages.shape}\")\n",
    "print(f\"Cleaned DataFrame shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22000597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "from typing import List, Dict, Any, Set, Tuple, Optional\n",
    "\n",
    "# Import tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "# tqdm.pandas() is used to add progress_apply to pandas Series/DataFrame\n",
    "# It needs to be initialized once.\n",
    "\n",
    "# Define a custom type for message dictionaries for clarity\n",
    "MessageType = Dict[str, Any]\n",
    "\n",
    "# Define the fields to be included in the output JSON\n",
    "SELECTED_JSON_FIELDS = ['id', 'date_unixtime', 'text', 'reply_to_message_id']\n",
    "\n",
    "def _default_json_serializer(obj: Any) -> str:\n",
    "    \"\"\"\n",
    "    Default JSON serializer for objects not handled by standard json.dumps.\n",
    "    This primarily handles datetime objects by converting them to ISO format strings.\n",
    "    \"\"\"\n",
    "    if hasattr(obj, 'isoformat'):  # Covers datetime.datetime, datetime.date\n",
    "        return obj.isoformat()\n",
    "    # For other non-serializable types, convert to string.\n",
    "    return str(obj)\n",
    "\n",
    "def _validate_input_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validates the input DataFrame, checks for required columns for threading logic,\n",
    "    and converts columns to expected types.\n",
    "    Returns a processed copy of the DataFrame if validation passes.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Required columns for the threading logic itself\n",
    "    required_columns_for_logic = ['id', 'reply_to_message_id', 'date_unixtime']\n",
    "    for col in required_columns_for_logic:\n",
    "        if col not in df_processed.columns:\n",
    "            raise ValueError(f\"Missing required column for threading logic: {col}\")\n",
    "\n",
    "    # Check for columns needed for the final JSON output.\n",
    "    # These are not strictly required for the threading algorithm to run,\n",
    "    # but if they are missing, they will be None in the output JSON.\n",
    "    # It's good practice to inform the user if they are missing.\n",
    "    for col in SELECTED_JSON_FIELDS:\n",
    "        if col not in df_processed.columns:\n",
    "            print(f\"Warning: Column '{col}' (selected for JSON output) not found in input DataFrame. \"\n",
    "                  f\"It will be missing or None in the output JSON for messages.\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        if df_processed['id'].isnull().any():\n",
    "            raise ValueError(\"Column 'id' contains NaN/null values.\")\n",
    "        temp_ids = df_processed['id'].astype('float').astype('Int64')\n",
    "        if temp_ids.isnull().any():\n",
    "            raise ValueError(\"Column 'id' contains values that cannot be robustly converted to integers.\")\n",
    "        df_processed['id'] = temp_ids.astype(int)\n",
    "    except (ValueError, TypeError, AttributeError) as e:\n",
    "        raise ValueError(f\"Column 'id' must contain unique, non-null values convertible to integers. Error: {e}\") from e\n",
    "\n",
    "    if not df_processed['id'].is_unique:\n",
    "        raise ValueError(\"Column 'id' must contain unique values.\")\n",
    "\n",
    "    try:\n",
    "        df_processed['reply_to_message_id'] = df_processed['reply_to_message_id'].astype('float').astype('Int64')\n",
    "    except (ValueError, TypeError) as e:\n",
    "        raise ValueError(f\"Column 'reply_to_message_id' contains values not convertible to nullable integers. Error: {e}\") from e\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(df_processed['date_unixtime']):\n",
    "        try:\n",
    "            df_processed['date_unixtime'] = pd.to_numeric(df_processed['date_unixtime'])\n",
    "        except (ValueError, TypeError) as e:\n",
    "            raise ValueError(f\"Column 'date_unixtime' must be numeric or convertible to numeric. Error: {e}\") from e\n",
    "    \n",
    "    if df_processed['date_unixtime'].isnull().any():\n",
    "        print(\"Info: 'date_unixtime' column contains NaN values. Messages with NaN timestamps will be sorted accordingly.\")\n",
    "        \n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def group_messages_into_threads(df: pd.DataFrame, min_messages_per_thread: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups messages from a DataFrame into conversational threads.\n",
    "\n",
    "    Each thread starts with a root message (one with no 'reply_to_message_id')\n",
    "    and includes all messages that directly or indirectly reply to it. Messages\n",
    "    within each thread are sorted chronologically by 'date_unixtime', then by 'id'.\n",
    "    Threads with fewer messages than `min_messages_per_thread` are excluded.\n",
    "    The output JSON for messages will only contain the fields specified in\n",
    "    `SELECTED_JSON_FIELDS`. Progress bars are displayed using tqdm.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with message data. Expected columns include:\n",
    "            'id': Unique integer identifier for the message.\n",
    "            'reply_to_message_id': Integer ID of the message this message\n",
    "                                   replies to (NaN, None, or pd.NA if not a reply).\n",
    "            'date_unixtime': Numeric timestamp (e.g., Unix time) for sorting and for output JSON.\n",
    "            'text': Text content of the message (for output JSON).\n",
    "            Other columns can be present but won't be in the final JSON unless\n",
    "            specified in `SELECTED_JSON_FIELDS`.\n",
    "        min_messages_per_thread (int): The minimum number of messages a thread\n",
    "                                       must contain to be included in the output.\n",
    "                                       Defaults to 2 (i.e., filter out single-message threads).\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame where each row represents a conversational thread.\n",
    "        Columns:\n",
    "            'root_message_id': The integer ID of the first message in the thread.\n",
    "            'messages_json': A JSON string representing a list of messages\n",
    "                             (as dictionaries with selected fields) in the thread, sorted.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If input DataFrame is malformed (e.g., missing columns,\n",
    "                    ID issues, type conversion problems) or if JSON serialization fails.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['root_message_id', 'messages_json'])\n",
    "\n",
    "    tqdm.pandas() # Initialize tqdm for pandas operations like progress_apply\n",
    "\n",
    "    df_processed = _validate_input_dataframe(df)\n",
    "\n",
    "    all_message_records = df_processed.to_dict(orient='records')\n",
    "    \n",
    "    messages_as_dicts: Dict[int, MessageType] = {}\n",
    "    for record in tqdm(all_message_records, desc=\"Indexing messages\", unit=\"msg\"):\n",
    "        messages_as_dicts[record['id']] = record\n",
    "\n",
    "    adj_list: Dict[int, List[int]] = collections.defaultdict(list)\n",
    "    root_message_ids: List[int] = []\n",
    "\n",
    "    for msg_id, msg_data in tqdm(messages_as_dicts.items(), desc=\"Building reply graph\", unit=\"msg\"):\n",
    "        parent_id_val = msg_data.get('reply_to_message_id')\n",
    "        # Ensure parent_id is an int if not pd.NA or None\n",
    "        parent_id: Optional[int] = None\n",
    "        if pd.notna(parent_id_val):\n",
    "            try:\n",
    "                parent_id = int(parent_id_val)\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Message ID {msg_id} has a non-integer 'reply_to_message_id' \"\n",
    "                      f\"value '{parent_id_val}'. This reply link will be ignored.\")\n",
    "                parent_id = None # Treat as no parent\n",
    "\n",
    "        if parent_id is None:\n",
    "            root_message_ids.append(msg_id)\n",
    "        else: # parent_id is a valid integer\n",
    "            adj_list[parent_id].append(msg_id)\n",
    "    \n",
    "    root_message_ids.sort()\n",
    "\n",
    "    threads_data: List[Dict[str, Any]] = []\n",
    "    processed_messages_globally: Set[int] = set()\n",
    "\n",
    "    for root_id in tqdm(root_message_ids, desc=\"Constructing threads\", unit=\"thread\"):\n",
    "        if root_id in processed_messages_globally:\n",
    "            # This case should ideally not happen if roots are truly roots and graph is a DAG collection\n",
    "            # print(f\"Warning: Root ID {root_id} was already processed as part of another thread. \"\n",
    "            #       \"This may indicate data issues. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        current_thread_messages_list: List[MessageType] = []\n",
    "        queue = collections.deque()\n",
    "        \n",
    "        if root_id not in messages_as_dicts: \n",
    "            print(f\"Critical Warning: Root ID {root_id} identified but not found in message map. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        queue.append(root_id)\n",
    "        visited_in_this_thread: Set[int] = set()\n",
    "\n",
    "        while queue:\n",
    "            current_id = queue.popleft()\n",
    "\n",
    "            if current_id in visited_in_this_thread:\n",
    "                # This can happen in non-tree structures (e.g. replies forming a DAG)\n",
    "                # print(f\"Info: Message ID {current_id} encountered again in thread for root {root_id}. Possible cycle or shared child.\")\n",
    "                continue\n",
    "            visited_in_this_thread.add(current_id)\n",
    "\n",
    "            if current_id in processed_messages_globally and current_id != root_id : # Allow root to be processed once\n",
    "                 print(f\"Warning: Message ID {current_id} (in thread of root {root_id}) was already assigned to another thread. \"\n",
    "                       \"It will be included in this thread as well. This indicates a message is a child in multiple threads (complex DAG).\")\n",
    "            processed_messages_globally.add(current_id)\n",
    "\n",
    "\n",
    "            message_data = messages_as_dicts.get(current_id)\n",
    "            if message_data:\n",
    "                current_thread_messages_list.append(message_data) # Store full message data for now\n",
    "                \n",
    "                children = sorted(adj_list.get(current_id, []))\n",
    "                for child_id in children:\n",
    "                    if child_id not in messages_as_dicts:\n",
    "                        print(f\"Warning: Message ID {child_id} (reply to {current_id}) not found in message data map. Skipping this child.\")\n",
    "                        continue\n",
    "                    if child_id not in visited_in_this_thread: # Add to queue only if not visited in current thread traversal\n",
    "                        queue.append(child_id)\n",
    "            else:\n",
    "                print(f\"Critical Warning: Message ID {current_id} was queued but not found in messages_as_dicts. Data inconsistency.\")\n",
    "\n",
    "        if current_thread_messages_list and len(current_thread_messages_list) >= min_messages_per_thread:\n",
    "            def sort_key_func(msg: MessageType) -> Tuple[Any, Any]:\n",
    "                ts_val = msg.get('date_unixtime')\n",
    "                id_val = msg.get('id')\n",
    "                # Handle potential NaN in timestamps by sorting them last or first as desired\n",
    "                # Here, NaN timestamps will sort after valid timestamps.\n",
    "                ts = ts_val if pd.notna(ts_val) else float('inf') \n",
    "                mid = id_val if pd.notna(id_val) else float('inf')\n",
    "                return (ts, mid)\n",
    "\n",
    "            try:\n",
    "                current_thread_messages_list.sort(key=sort_key_func)\n",
    "            except TypeError as e:\n",
    "                print(f\"Warning: Could not sort messages for thread rooted at {root_id} due to \"\n",
    "                      f\"type errors in sort keys ({e}). Messages may be in BFS discovery order or partially sorted.\")\n",
    "            \n",
    "            threads_data.append({\n",
    "                'root_message_id': root_id,\n",
    "                'messages': current_thread_messages_list # Still full messages here\n",
    "            })\n",
    "        elif current_thread_messages_list:\n",
    "            # Optionally log this, or just silently filter\n",
    "            # print(f\"Info: Thread rooted at {root_id} has {len(current_thread_messages_list)} messages, \"\n",
    "            #       f\"which is less than min_messages_per_thread={min_messages_per_thread}. Skipping.\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    all_df_ids = set(df_processed['id'])\n",
    "    # Messages that are roots of threads smaller than min_messages_per_thread will also be in orphaned_ids\n",
    "    # if they were not part of any larger, processed thread.\n",
    "    orphaned_ids = all_df_ids - processed_messages_globally\n",
    "    if orphaned_ids:\n",
    "        print(f\"Info: {len(orphaned_ids)} message(s) were not part of any qualifying thread \"\n",
    "              f\"(e.g., standalone messages, or roots of threads smaller than {min_messages_per_thread} messages). \"\n",
    "              f\"First 10 such IDs: {list(orphaned_ids)[:10]}...\")\n",
    "\n",
    "    if not threads_data:\n",
    "        return pd.DataFrame(columns=['root_message_id', 'messages_json'])\n",
    "\n",
    "    result_df = pd.DataFrame(threads_data)\n",
    "    \n",
    "    if result_df.empty: # Check if DataFrame is empty after filtering by min_messages_per_thread\n",
    "        return pd.DataFrame(columns=['root_message_id', 'messages_json'])\n",
    "        \n",
    "    try:\n",
    "        tqdm.pandas(desc=\"Serializing threads to JSON with selected fields\")\n",
    "        \n",
    "        def create_json_with_selected_fields(msg_list: List[MessageType]) -> str:\n",
    "            selected_field_msg_list = []\n",
    "            for msg_dict in msg_list:\n",
    "                # Create a new dictionary with only the selected fields\n",
    "                # .get(key) will return None if the key is missing, which is fine.\n",
    "                filtered_msg = {key: msg_dict.get(key) for key in SELECTED_JSON_FIELDS}\n",
    "                selected_field_msg_list.append(filtered_msg)\n",
    "            return json.dumps(selected_field_msg_list, ensure_ascii=False, default=_default_json_serializer)\n",
    "\n",
    "        result_df['messages_json'] = result_df['messages'].progress_apply(create_json_with_selected_fields)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to serialize messages to JSON. Error: {e}\") from e\n",
    "    \n",
    "    return result_df[['root_message_id', 'messages_json']]\n",
    "\n",
    "\n",
    "df_message_threads = group_messages_into_threads(df_cleaned)\n",
    "print(df_message_threads.head())\n",
    "if not df_message_threads.empty:\n",
    "    print(\"\\nExample JSON output for the first thread:\")\n",
    "    print(json.dumps(json.loads(df_message_threads.iloc[0]['messages_json']), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def group_messages_into_threads(df_input_messages: pd.DataFrame, time_threshold_seconds: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes rows of messages, grouping them into logical threads based on a time heuristic.\n",
    "\n",
    "    Args:\n",
    "        df_input_messages: DataFrame with a 'messages_json' column. Each entry in\n",
    "                           'messages_json' is a JSON string representing a list of\n",
    "                           message objects. Each message object is expected to have 'id',\n",
    "                           'date_unixtime', and optionally 'reply_to_message_id'.\n",
    "        time_threshold_seconds: If a new row's first message is a \"root\" message\n",
    "                                (no reply_to_message_id) and appears chronologically\n",
    "                                after the last message of the current accumulated thread\n",
    "                                and within this time threshold (in seconds),\n",
    "                                it's considered a continuation of that thread.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with columns \"root_message_id\" and \"messages_json\".\n",
    "        \"root_message_id\" is the ID of the first message in the logical thread.\n",
    "        \"messages_json\" is a JSON string of all messages in that logical thread.\n",
    "        Returns an empty DataFrame with these columns if input is empty or no valid threads are formed.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_threads_data = [] \n",
    "    current_accumulated_messages = []\n",
    "\n",
    "    output_columns = [\"root_message_id\", \"messages_json\"]\n",
    "\n",
    "    if df_input_messages.empty:\n",
    "        return pd.DataFrame(columns=output_columns)\n",
    "\n",
    "    for _, row_series in df_input_messages.iterrows():\n",
    "        json_string = row_series.get('messages_json')\n",
    "        \n",
    "        if not isinstance(json_string, str):\n",
    "            # Skip row if 'messages_json' is not a string\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            messages_in_row = json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip row if JSON is malformed\n",
    "            continue\n",
    "\n",
    "        if not isinstance(messages_in_row, list) or not messages_in_row:\n",
    "            # Skip row if parsed JSON is not a list or is an empty list\n",
    "            continue\n",
    "        \n",
    "        if not current_accumulated_messages:\n",
    "            # Ensure the first message of a new accumulation is valid before extending\n",
    "            if isinstance(messages_in_row[0], dict) and \\\n",
    "               'id' in messages_in_row[0] and \\\n",
    "               'date_unixtime' in messages_in_row[0]:\n",
    "                current_accumulated_messages.extend(messages_in_row)\n",
    "            else:\n",
    "                # First row's data is invalid for starting a thread, skip\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                # Ensure first message of current row and last of accumulated are valid dicts with keys\n",
    "                if not (isinstance(messages_in_row[0], dict) and \\\n",
    "                        isinstance(current_accumulated_messages[-1], dict) and \\\n",
    "                        'date_unixtime' in messages_in_row[0] and \\\n",
    "                        'date_unixtime' in current_accumulated_messages[-1]):\n",
    "                    raise ValueError(\"Invalid message structure for comparison\")\n",
    "\n",
    "\n",
    "                first_message_this_row = messages_in_row[0]\n",
    "                last_message_previous_accumulation = current_accumulated_messages[-1]\n",
    "                \n",
    "                time_difference = first_message_this_row['date_unixtime'] - last_message_previous_accumulation['date_unixtime']\n",
    "                is_starter_message = first_message_this_row.get(\"reply_to_message_id\") is None\n",
    "                \n",
    "            except (KeyError, TypeError, IndexError, ValueError):\n",
    "                # Data integrity issue. Finalize the previous thread (if valid) and start a new one.\n",
    "                if current_accumulated_messages: # Finalize previous thread\n",
    "                    # current_accumulated_messages[0] should be valid if it started a thread\n",
    "                    if isinstance(current_accumulated_messages[0], dict) and 'id' in current_accumulated_messages[0]:\n",
    "                        root_id = current_accumulated_messages[0]['id']\n",
    "                        messages_json_str = json.dumps(current_accumulated_messages, ensure_ascii=False)\n",
    "                        processed_threads_data.append({\n",
    "                            \"root_message_id\": root_id,\n",
    "                            \"messages_json\": messages_json_str\n",
    "                        })\n",
    "                \n",
    "                # Attempt to start new thread with current row's messages, if valid\n",
    "                if isinstance(messages_in_row[0], dict) and \\\n",
    "                   'id' in messages_in_row[0] and \\\n",
    "                   'date_unixtime' in messages_in_row[0]:\n",
    "                    current_accumulated_messages = messages_in_row\n",
    "                else:\n",
    "                    current_accumulated_messages = [] # Reset, current row also invalid\n",
    "                continue\n",
    "\n",
    "            is_chronological_and_within_threshold = (0 <= time_difference < time_threshold_seconds)\n",
    "\n",
    "            if is_starter_message and is_chronological_and_within_threshold:\n",
    "                current_accumulated_messages.extend(messages_in_row)\n",
    "            else:\n",
    "                # Finalize the previous logical thread\n",
    "                # current_accumulated_messages[0] is known to be valid from when it started accumulation\n",
    "                if current_accumulated_messages:\n",
    "                    root_id = current_accumulated_messages[0]['id'] \n",
    "                    messages_json_str = json.dumps(current_accumulated_messages, ensure_ascii=False)\n",
    "                    processed_threads_data.append({\n",
    "                        \"root_message_id\": root_id,\n",
    "                        \"messages_json\": messages_json_str\n",
    "                    })\n",
    "                \n",
    "                # Start new thread if current row's first message is valid\n",
    "                if isinstance(messages_in_row[0], dict) and \\\n",
    "                   'id' in messages_in_row[0] and \\\n",
    "                   'date_unixtime' in messages_in_row[0]:\n",
    "                    current_accumulated_messages = messages_in_row\n",
    "                else:\n",
    "                    current_accumulated_messages = [] # Reset, current row invalid to start new thread\n",
    "\n",
    "    # Add the last accumulated thread\n",
    "    if current_accumulated_messages:\n",
    "        # current_accumulated_messages[0] is known to be valid if list is not empty\n",
    "        if isinstance(current_accumulated_messages[0], dict) and 'id' in current_accumulated_messages[0]:\n",
    "            root_id = current_accumulated_messages[0]['id']\n",
    "            messages_json_str = json.dumps(current_accumulated_messages, ensure_ascii=False)\n",
    "            processed_threads_data.append({\n",
    "                \"root_message_id\": root_id,\n",
    "                \"messages_json\": messages_json_str\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(processed_threads_data, columns=output_columns)\n",
    "\n",
    "df_message_threads = group_messages_into_threads(df_message_threads, 2*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define question starters and greeting patterns\n",
    "QUESTION_STARTERS_RU = [\n",
    "    \"кто\", \"что\", \"где\", \"когда\", \"как\", \"почему\", \"зачем\", \"сколько\", \"какой\",\n",
    "    \"какая\", \"какое\", \"какие\", \"чей\", \"чья\", \"чьё\", \"чьи\", \"расскажите\",\n",
    "    \"подскажите\", \"посоветуйте\", \"знает ли\", \"знаете ли\", \"кто-нибудь знает\",\n",
    "    \"не подскажете\", \"не знаете ли\", \"можно ли\", \"есть ли\", \"верно ли\",\n",
    "    \"правильно ли\", \"нужен ли\", \"нужна ли\", \"нужно ли\", \"нужны ли\",\n",
    "    \"а где\", \"а как\", \"а кто\", \"а что\", \"а когда\", \"а сколько\", \"во сколько\",\n",
    "    \"за сколько\", \"каков\", \"какова\", \"каково\", \"каковы\"\n",
    "]\n",
    "\n",
    "GREETING_PATTERN_STR = r\"^(?:привет\\s*всем|всем\\s*привет|привет|здравствуйте|добрый\\s*(?:день|вечер|утро)|доброго\\s*времени\\s*суток|ребят(?:а)?|девочки|парни|друзья|господа|народ|пацаны|мужики|коллеги|товарищи|всем\\s*доброго\\s*вечера|всем\\s*добрый)\\s*[,!)\\s]*\"\n",
    "GREETING_PATTERN = re.compile(GREETING_PATTERN_STR)\n",
    "\n",
    "def is_meaningful_question(text_content: any) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the given text content of a message is a meaningful question.\n",
    "    Aims to identify genuine inquiries rather than simple greetings or statements.\n",
    "    \"\"\"\n",
    "    if not isinstance(text_content, str):\n",
    "        return False\n",
    "\n",
    "    original_text_lower_stripped = text_content.lower().strip()\n",
    "    \n",
    "    # Filter out very short messages that are unlikely to be meaningful questions,\n",
    "    # unless they are a direct question starter or a single interrogative word.\n",
    "    if len(original_text_lower_stripped) < 3 and original_text_lower_stripped != \"?\":\n",
    "        # Allow single-word question starters like \"кто?\", \"как?\"\n",
    "        is_starter_word = any(original_text_lower_stripped.rstrip(\"?!.,\") == qs for qs in QUESTION_STARTERS_RU)\n",
    "        if not is_starter_word:\n",
    "             return False\n",
    "\n",
    "    text_after_greeting = GREETING_PATTERN.sub(\"\", original_text_lower_stripped, count=1).strip()\n",
    "    \n",
    "    if not text_after_greeting: # If only greeting was present or became empty\n",
    "        return False\n",
    "\n",
    "    text_to_analyze_starters = text_after_greeting\n",
    "\n",
    "    # Check 1: Starts with a question starter word/phrase\n",
    "    for starter in QUESTION_STARTERS_RU:\n",
    "        if text_to_analyze_starters.startswith(starter):\n",
    "            # Avoid known non-question phrases like \"как говорится\"\n",
    "            if text_to_analyze_starters.startswith(\"как говорится\"):\n",
    "                return False\n",
    "            \n",
    "            # Prioritize if it also ends with a question mark or is relatively short (more likely a direct question)\n",
    "            if original_text_lower_stripped.endswith(\"?\") or len(text_to_analyze_starters.split()) <= 7:\n",
    "                 return True\n",
    "            # If longer and no QM, but starts with a strong question starter, still consider it a question.\n",
    "            # This heuristic can be adjusted based on desired strictness.\n",
    "            # For this task, starting with a question word is a strong indicator.\n",
    "            return True \n",
    "            \n",
    "    # Check 2: Ends with a question mark and is not too short or a simple exclamation/interjection\n",
    "    if original_text_lower_stripped.endswith(\"?\"):\n",
    "        words = original_text_lower_stripped.split()\n",
    "        if not words: \n",
    "            return False\n",
    "\n",
    "        # Avoid single interjections like \"Ого?\", \"А?\"\n",
    "        if len(words) == 1:\n",
    "            first_word_original_cleaned = words[0].rstrip('?!.,')\n",
    "            # Allow single interrogative words like \"Можно?\", \"Когда?\"\n",
    "            if first_word_original_cleaned in QUESTION_STARTERS_RU:\n",
    "                return True\n",
    "            # Allow other single words if they are reasonably long (e.g. \"Помощь?\")\n",
    "            if len(first_word_original_cleaned) > 3 and first_word_original_cleaned not in [\"ого\", \"а\", \"ну\", \"да\", \"нет\", \"ок\"]:\n",
    "                return True\n",
    "            return False # Short, non-starter single words like \"Эй?\" are not meaningful questions\n",
    "\n",
    "        # For multi-word messages ending in '?'\n",
    "        first_word_original_cleaned = words[0].rstrip('?!.,')\n",
    "        # Filter out interjections or very common non-question starters\n",
    "        if first_word_original_cleaned not in [\"ого\", \"ок\", \"да\", \"нет\", \"ладно\", \"ага\", \"ну\", \"привет\", \"здравствуйте\"]:\n",
    "             if len(original_text_lower_stripped) > 5: # Arbitrary length to avoid very short, ambiguous phrases\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def filter_dataframe_for_meaningful_question_threads(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the input DataFrame to keep only threads where the first message\n",
    "    is identified as a meaningful question.\n",
    "    \"\"\"\n",
    "    if df_input.empty:\n",
    "        return df_input.copy()\n",
    "\n",
    "    # Create a boolean mask for filtering\n",
    "    rows_to_keep_mask = pd.Series([False] * len(df_input), index=df_input.index)\n",
    "    \n",
    "    for index, row in df_input.iterrows():\n",
    "        try:\n",
    "            # Load the thread messages from the JSON string\n",
    "            thread_messages = json.loads(row['messages_json'])\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip rows with malformed JSON\n",
    "            continue\n",
    "\n",
    "        if not thread_messages or not isinstance(thread_messages, list) or \\\n",
    "           not thread_messages[0] or not isinstance(thread_messages[0], dict):\n",
    "            # Skip if thread is empty or first message is invalid\n",
    "            continue\n",
    "            \n",
    "        # The first message in the list is considered the starting message of the thread\n",
    "        starting_message = thread_messages[0]\n",
    "        \n",
    "        text_content = starting_message.get('text')\n",
    "        \n",
    "        if is_meaningful_question(text_content):\n",
    "            rows_to_keep_mask.loc[index] = True\n",
    "            \n",
    "    return df_input[rows_to_keep_mask].copy()\n",
    "\n",
    "df_question_threads = filter_dataframe_for_meaningful_question_threads(df_message_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1df77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_threads.iloc[0].messages_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbba7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_threads.to_csv(\"chat_threads.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
